{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo8WznxNdqD9"
      },
      "source": [
        "#  <center> Problem Set 6 <center>\n",
        "\n",
        "<center> 3.C01/3.C51, 10.C01/10.C51 <center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cheminformatics\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem, Descriptors,Crippen\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "# Arrays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Plotting \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# Personal utility package\n",
        "import package.plot\n",
        "from package.plot import get_size_inches\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Machine learning\n",
        "import torch\n",
        "from lightning import pytorch as pl\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "# 10.1021/acs.jcim.3c01250\n",
        "from chemprop import data, featurizers, models, nn\n",
        "\n",
        "# Utility\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkB_bDgydqEJ"
      },
      "source": [
        "## Part 1: Baseline Regression Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZBbezSgeV9i"
      },
      "source": [
        "### Part 1.1: (5 points) Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path.cwd() / \"data\"\n",
        "train_file = data_dir / \"solvation_train.csv\"\n",
        "test_file = data_dir / \"solvation_test.csv\"\n",
        "prop_file = data_dir / \"molecule_props.csv\"\n",
        "df = pd.read_csv(train_file) # load data\n",
        "mol_prop = pd.read_csv(prop_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMYXzNLCepen"
      },
      "source": [
        "Generate fingerprints (e.g. a Morgan fingerprint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zL8tJad6dqEU",
        "outputId": "20f7560c-8a7b-4c8c-b835-1b90056bb46a"
      },
      "outputs": [],
      "source": [
        "smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'\n",
        "mol = Chem.MolFromSmiles(smiles) # load SMILES into RDKit\n",
        "fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=512) # morgan fingerprint \n",
        "fp_array = np.zeros((1,), int) # convert to numpy array\n",
        "DataStructs.ConvertToNumpyArray(fp, fp_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tItagId2erRw"
      },
      "source": [
        "Generate various chemical properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jU2HiX0hdqEZ",
        "outputId": "9814ead6-e570-4d4d-800f-723e094e71b9"
      },
      "outputs": [],
      "source": [
        "MolWt = Descriptors.ExactMolWt(mol) # molecular weight\n",
        "TPSA = Chem.rdMolDescriptors.CalcTPSA(mol) # Topological Polar Surface Area\n",
        "nRotB = Descriptors.NumRotatableBonds(mol) # number of rotable bonds\n",
        "HBD = Descriptors.NumHDonors(mol) # number of H bond donors\n",
        "HBA = Descriptors.NumHAcceptors(mol) # number of H bond acceptors\n",
        "logP = Descriptors.MolLogP(mol) # LogP\n",
        "dct_pol = dict(zip(mol_prop[\"SMILES\"],mol_prop[\"polarizability\"]))\n",
        "dct_dip = dict(zip(mol_prop[\"SMILES\"],mol_prop[\"dipole\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkl6NBhJew2K"
      },
      "source": [
        "Create a feature set with concatenated physical descriptors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uj-SFjPee0My"
      },
      "outputs": [],
      "source": [
        "def featurize(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles) # load SMILES into RDKit\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=512) # morgan fingerprint \n",
        "    fp_array = np.zeros((1,), int) # convert to numpy array\n",
        "    DataStructs.ConvertToNumpyArray(fp, fp_array)\n",
        "    \n",
        "    MolWt = Descriptors.ExactMolWt(mol) # molecular weight\n",
        "    TPSA = Chem.rdMolDescriptors.CalcTPSA(mol) # Topological Polar Surface Area\n",
        "    nRotB = Descriptors.NumRotatableBonds(mol) # number of rotable bonds\n",
        "    HBD = Descriptors.NumHDonors(mol) # number of H bond donors\n",
        "    HBA = Descriptors.NumHAcceptors(mol) # number of H bond acceptors\n",
        "    logP = Descriptors.MolLogP(mol) # LogP\n",
        "    # polar = dct_pol[smiles]\n",
        "    # dipole = dct_dip[smiles]\n",
        "\n",
        "    return np.hstack([fp_array, [MolWt, TPSA, nRotB, HBD, HBA, logP]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = df[\"logK\"]\n",
        "X_solvent = np.stack(df[\"Solvent\"].apply(featurize).values)\n",
        "X_solute = np.stack(df[\"Solute\"].apply(featurize).values)\n",
        "X = np.hstack([X_solvent,X_solute])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yss40IY_dqEN"
      },
      "source": [
        "### 1.2 (10 points) Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ixgSvHFhao0"
      },
      "source": [
        "Train a linear regression model and report a 5-fold cross-validated R^2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mpkBJk5cdqEP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Regularization R2: -1.0751368220270634e+16\n",
            "Lasso R2: 0.4794894887076525\n",
            "Ridge R2: 0.9224699304617175\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "scores = cross_val_score(LinearRegression(), X, y, cv = 5, scoring = \"r2\")\n",
        "print(f\"No Regularization R2: {scores.mean()}\")\n",
        "scores = cross_val_score(Lasso(), X, y, cv = 5, scoring = \"r2\")\n",
        "print(f\"Lasso R2: {scores.mean()}\")\n",
        "scores = cross_val_score(Ridge(), X, y, cv = 5, scoring = \"r2\")\n",
        "print(f\"Ridge R2: {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-laiNUcodqEP"
      },
      "source": [
        "### 1.3 (10 points) MLP Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meAJkhi4hhuT"
      },
      "source": [
        "Train an MLP regression model and report a 5-fold cross-validated R^2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J3ibqCdxdqER"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP R2: 0.9643595568587566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# initialize a MLP with the specified hyperparameters\n",
        "kwargs = {\"hidden_layer_sizes\" : (256,256,256),\n",
        "          \"activation\" : \"relu\",\n",
        "          \"alpha\" : 0.16,\n",
        "          \"solver\" : \"adam\",\n",
        "          \"early_stopping\" : False}\n",
        "mlp =  MLPRegressor(**kwargs)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('model', mlp)])\n",
        "scores = cross_val_score(pipe, X, y, cv = 5, scoring = \"r2\")\n",
        "print(f\"MLP R2: {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npN-qSQDdqER"
      },
      "source": [
        "## Part 2: (50 points) Machine Learning Competition and Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J34YVX-nkoJd"
      },
      "source": [
        "You can start a new notebook here to put all your models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "j3B9eYwVdqEd"
      },
      "outputs": [],
      "source": [
        "def save_submission(prediction, filename):\n",
        "    '''\n",
        "    Utility function to dump a submission file.\n",
        "\n",
        "    prediction (numpy.array): 1d numpy array contains your prediction\n",
        "    filename (str): file path to where you want to save the result\n",
        "    '''\n",
        "    sub = pd.DataFrame( {'index': list(range(len(prediction))), 'logK': prediction } )\n",
        "    sub.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PropFeaturizer(featurizers.Featurizer):\n",
        "    def __init__(self, features: list):\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.features)\n",
        "\n",
        "    def MolWt(self, mol):\n",
        "        return Descriptors.ExactMolWt(mol)\n",
        "\n",
        "    def TPSA(self, mol):\n",
        "        return Chem.rdMolDescriptors.CalcTPSA(mol)\n",
        "    \n",
        "    def LASA(self, mol):\n",
        "        return Chem.rdMolDescriptors.CalcLabuteASA(mol)\n",
        "\n",
        "    def NumRotatableBonds(self, mol):\n",
        "        return Descriptors.NumRotatableBonds(mol)\n",
        "\n",
        "    def NumHDonors(self, mol):\n",
        "        return Descriptors.NumHDonors(mol)\n",
        "\n",
        "    def NumHAcceptors(self, mol):\n",
        "        return Descriptors.NumHAcceptors(mol)\n",
        "\n",
        "    def MolLogP(self, mol):\n",
        "        return Descriptors.MolLogP(mol)\n",
        "\n",
        "    def MolMR(self, mol):\n",
        "        return Descriptors.MolMR(mol)\n",
        "\n",
        "    def AromProp(self, mol):\n",
        "        return len(list(mol.GetAromaticAtoms())) / mol.GetNumHeavyAtoms()\n",
        "\n",
        "    def __call__(self, mol: Chem.Mol) -> np.ndarray:\n",
        "        return np.array([getattr(self,feature)(mol) for feature in self.features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chemprop.data.collate import NamedTuple, BatchMolGraph, Tensor, Iterable, Datum, collate_batch\n",
        "from chemprop.data.dataloader import MoleculeDataset, ReactionDataset, MulticomponentDataset, DataLoader\n",
        "import warnings\n",
        "\n",
        "class MulticomponentTrainingBatch(NamedTuple):\n",
        "    bmgs: list[BatchMolGraph]\n",
        "    V_ds: list[Tensor | None]\n",
        "    X_d: Tensor | None\n",
        "    Y: Tensor | None\n",
        "    w: Tensor\n",
        "    lt_mask: Tensor | None\n",
        "    gt_mask: Tensor | None\n",
        "\n",
        "\n",
        "def custom_collate_multicomponent(batches: Iterable[Iterable[Datum]]) -> MulticomponentTrainingBatch:\n",
        "    tbs = [collate_batch(batch) for batch in zip(*batches)]\n",
        "    return MulticomponentTrainingBatch(\n",
        "        [tb.bmg for tb in tbs],\n",
        "        [tb.V_d for tb in tbs],\n",
        "        torch.cat([tbs[0].X_d,tbs[1].X_d],axis=1),\n",
        "        tbs[0].Y,\n",
        "        tbs[0].w,\n",
        "        tbs[0].lt_mask,\n",
        "        tbs[0].gt_mask,\n",
        "    )\n",
        "\n",
        "def custom_build_dataloader(\n",
        "    dataset: MoleculeDataset | ReactionDataset | MulticomponentDataset,\n",
        "    batch_size: int = 64,\n",
        "    num_workers: int = 0,\n",
        "    class_balance: bool = False,\n",
        "    seed: int | None = None,\n",
        "    shuffle: bool = True,\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "    if class_balance:\n",
        "        sampler = ClassBalanceSampler(dataset.Y, seed, shuffle)\n",
        "    elif shuffle and seed is not None:\n",
        "        sampler = SeededSampler(len(dataset), seed)\n",
        "    else:\n",
        "        sampler = None\n",
        "\n",
        "    if isinstance(dataset, MulticomponentDataset):\n",
        "        collate_fn = custom_collate_multicomponent\n",
        "    else:\n",
        "        collate_fn = collate_batch\n",
        "\n",
        "    if len(dataset) % batch_size == 1:\n",
        "        warnings.warn(\n",
        "            f\"Dropping last batch of size 1 to avoid issues with batch normalization \\\n",
        "(dataset size = {len(dataset)}, batch_size = {batch_size})\"\n",
        "        )\n",
        "        drop_last = True\n",
        "    else:\n",
        "        drop_last = False\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size,\n",
        "        sampler is None and shuffle,\n",
        "        sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=drop_last,\n",
        "        **kwargs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = [\"TPSA\", \"LASA\", \"NumRotatableBonds\", \"NumHDonors\", \"NumHAcceptors\", \"MolLogP\", \"MolMR\", \"AromProp\"]\n",
        "\n",
        "def get_loaders(features, split_type=\"random\", split = (0.8, 0.1, 0.1)):\n",
        "    data_dir = Path.cwd() / \"data\"\n",
        "    train_file = data_dir / \"solvation_train.csv\"\n",
        "    test_file = data_dir / \"solvation_test.csv\"\n",
        "    prop_file = data_dir / \"molecule_props.csv\"\n",
        "    smiles_columns = ['Solute', 'Solvent'] # name of the column containing SMILES strings\n",
        "    target_columns = ['logK'] # list of names of the columns containing targets\n",
        "    df_input = pd.read_csv(train_file)\n",
        "    smiss = df_input.loc[:, smiles_columns].values\n",
        "    ys = df_input.loc[:, target_columns].values\n",
        "\n",
        "    mfs = [PropFeaturizer(features)]\n",
        "    all_data = [[data.MoleculeDatapoint.from_smi(smis[0], y, mfs=mfs) for smis, y in zip(smiss, ys)]]\n",
        "    all_data += [[data.MoleculeDatapoint.from_smi(smis[i], mfs=mfs) for smis in smiss] for i in range(1, len(smiles_columns))]\n",
        "\n",
        "    component_to_split_by = 0\n",
        "    mols = [d.mol for d in all_data[component_to_split_by]]\n",
        "\n",
        "    train_indices, val_indices, test_indices = data.make_split_indices(mols, split_type, split)\n",
        "    \n",
        "    train_data, val_data, test_data = data.split_data_by_indices(\n",
        "        all_data, train_indices, val_indices, test_indices\n",
        "    )\n",
        "\n",
        "    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
        "\n",
        "    train_datasets = [data.MoleculeDataset(train_data[i], featurizer) for i in range(len(smiles_columns))]\n",
        "    train_mcdset = data.MulticomponentDataset(train_datasets)\n",
        "    scaler = train_mcdset.normalize_targets()\n",
        "    train_loader = custom_build_dataloader(train_mcdset)\n",
        "\n",
        "    val_datasets = [data.MoleculeDataset(val_data[i], featurizer) for i in range(len(smiles_columns))]\n",
        "    val_mcdset = data.MulticomponentDataset(val_datasets)\n",
        "    val_mcdset.normalize_targets(scaler)\n",
        "    val_loader = custom_build_dataloader(val_mcdset, shuffle=False)\n",
        "\n",
        "    test_datasets = [data.MoleculeDataset(test_data[i], featurizer) for i in range(len(smiles_columns))]\n",
        "    test_mcdset = data.MulticomponentDataset(test_datasets)\n",
        "    test_loader = custom_build_dataloader(test_mcdset, shuffle=False)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2407107/1504296857.py:50: UserWarning: Dropping last batch of size 1 to avoid issues with batch normalization (dataset size = 1, batch_size = 64)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader, test_loader = get_loaders(features, split = (0.99, 0.01, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'output_transform' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['output_transform'])`.\n"
          ]
        }
      ],
      "source": [
        "mcmp = nn.MulticomponentMessagePassing(\n",
        "    blocks=[nn.BondMessagePassing(depth=6) for _ in range(len(smiles_columns))],\n",
        "    n_components=len(smiles_columns),\n",
        ")\n",
        "\n",
        "agg = nn.MeanAggregation()\n",
        "\n",
        "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
        "ffn = nn.RegressionFFN(\n",
        "    input_dim=mcmp.output_dim + (len(smiles_columns)) * np.sum([i.__len__() for i in mfs]),\n",
        "    output_transform=output_transform,\n",
        "    hidden_dim=1900,\n",
        "    n_layers=2,\n",
        "    dropout=0.008,\n",
        ")\n",
        "\n",
        "metric_list = [nn.metrics.RMSEMetric(), nn.metrics.MAEMetric(), nn.metrics.R2Metric()] # Only the first metric is used for training and early stopping\n",
        "\n",
        "mcmpnn = models.multi.MulticomponentMPNN(\n",
        "    mcmp,\n",
        "    agg,\n",
        "    ffn,\n",
        "    metrics=metric_list,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/gridsan/ddavid/.conda/envs/torch/lib/python3.1 ...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n",
            "\n",
            "  | Name            | Type                         | Params\n",
            "-----------------------------------------------------------------\n",
            "0 | message_passing | MulticomponentMessagePassing | 455 K \n",
            "1 | agg             | MeanAggregation              | 0     \n",
            "2 | bn              | BatchNorm1d                  | 1.2 K \n",
            "3 | predictor       | RegressionFFN                | 4.8 M \n",
            "4 | X_d_transform   | Identity                     | 0     \n",
            "-----------------------------------------------------------------\n",
            "5.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.2 M     Total params\n",
            "20.971    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 103: 100%|██████████| 57/57 [00:01<00:00, 37.35it/s, v_num=2, train_loss=0.0035, val_loss=0.0666] "
          ]
        }
      ],
      "source": [
        "logger = CSVLogger(\"logs\", name=\"model\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    enable_checkpointing=True,\n",
        "    enable_progress_bar=True,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    max_epochs=200, # number of epochs to train for\n",
        ")\n",
        "\n",
        "trainer.fit(mcmpnn, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/gridsan/ddavid/.conda/envs/torch/lib/python3.1 ...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        }
      ],
      "source": [
        "results = trainer.test(mcmpnn, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kaggle_path = data_dir / \"solvation_test.csv\"\n",
        "df_kaggle = pd.read_csv(kaggle_path)\n",
        "smiss_kaggle = df_kaggle.loc[:, smiles_columns].values\n",
        "kaggle_data = [[data.MoleculeDatapoint.from_smi(smis[i], mfs=mfs) for smis in smiss_kaggle] for i in range(len(smiles_columns))]\n",
        "kaggle_datasets = [data.MoleculeDataset(kaggle_data[i], featurizer) for i in range(len(smiles_columns))]\n",
        "kaggle_mcdset = data.MulticomponentDataset(kaggle_datasets)\n",
        "kaggle_loader = custom_build_dataloader(kaggle_mcdset,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/gridsan/ddavid/.conda/envs/torch/lib/python3.1 ...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|██████████| 25/25 [00:00<00:00, 46.19it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Solvent</th>\n",
              "      <th>Solute</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>CCCCCO</td>\n",
              "      <td>[Ne]</td>\n",
              "      <td>-1.531519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>CC(C)CC(C)(C)C</td>\n",
              "      <td>CCCCO</td>\n",
              "      <td>2.774545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>c1ccc(cc1)[N+](=O)[O-]</td>\n",
              "      <td>Cc1ccccc1</td>\n",
              "      <td>3.344532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>C/C(=N/C)/O</td>\n",
              "      <td>CC(C)O</td>\n",
              "      <td>3.661321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>CCOC(=O)C</td>\n",
              "      <td>CCCCCC</td>\n",
              "      <td>2.459938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1564</th>\n",
              "      <td>1564</td>\n",
              "      <td>CCCCCC</td>\n",
              "      <td>c1ccc(c(c1)Cl)Cl</td>\n",
              "      <td>4.454704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1565</th>\n",
              "      <td>1565</td>\n",
              "      <td>CC(C)(C)O</td>\n",
              "      <td>C(=O)=O</td>\n",
              "      <td>0.411111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>1566</td>\n",
              "      <td>CCCOCC</td>\n",
              "      <td>CO</td>\n",
              "      <td>2.276852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1567</th>\n",
              "      <td>1567</td>\n",
              "      <td>CCCCCCC</td>\n",
              "      <td>CCCC=C</td>\n",
              "      <td>2.288224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1568</th>\n",
              "      <td>1568</td>\n",
              "      <td>CCCCCCCC</td>\n",
              "      <td>Cc1ccc(cc1)N</td>\n",
              "      <td>4.533404</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1569 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index                 Solvent            Solute      pred\n",
              "0         0                  CCCCCO              [Ne] -1.531519\n",
              "1         1          CC(C)CC(C)(C)C             CCCCO  2.774545\n",
              "2         2  c1ccc(cc1)[N+](=O)[O-]         Cc1ccccc1  3.344532\n",
              "3         3             C/C(=N/C)/O            CC(C)O  3.661321\n",
              "4         4               CCOC(=O)C            CCCCCC  2.459938\n",
              "...     ...                     ...               ...       ...\n",
              "1564   1564                  CCCCCC  c1ccc(c(c1)Cl)Cl  4.454704\n",
              "1565   1565               CC(C)(C)O           C(=O)=O  0.411111\n",
              "1566   1566                  CCCOCC                CO  2.276852\n",
              "1567   1567                 CCCCCCC            CCCC=C  2.288224\n",
              "1568   1568                CCCCCCCC      Cc1ccc(cc1)N  4.533404\n",
              "\n",
              "[1569 rows x 4 columns]"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_preds = trainer.predict(mcmpnn, kaggle_loader)\n",
        "test_preds = np.concatenate(test_preds, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "formatted_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "save_submission(test_preds,f\"kaggle/chemprop_{formatted_time}.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pset6_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
