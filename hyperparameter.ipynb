{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Cheminformatics\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors,Crippen\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Personal utility package\n",
    "import package.plot\n",
    "from package.plot import get_size_inches\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# 10.1021/acs.jcim.3c01250\n",
    "from chemprop import data, featurizers, models, nn\n",
    "from chemprop.data.collate import NamedTuple, BatchMolGraph, Tensor, Iterable, Datum, collate_batch\n",
    "from chemprop.data.dataloader import MoleculeDataset, ReactionDataset, MulticomponentDataset, DataLoader\n",
    "\n",
    "# Utility\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "from optuna_integration import PyTorchLightningPruningCallback\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "def save_submission(prediction, filename):\n",
    "    '''\n",
    "    Utility function to dump a submission file.\n",
    "\n",
    "    prediction (numpy.array): 1d numpy array contains your prediction\n",
    "    filename (str): file path to where you want to save the result\n",
    "    '''\n",
    "    sub = pd.DataFrame( {'index': list(range(len(prediction))), 'logK': prediction } )\n",
    "    sub.to_csv(filename, index=False)\n",
    "\n",
    "@dataclass\n",
    "class PropFeaturizer(featurizers.Featurizer):\n",
    "    def __init__(self, features: list):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def MolWt(self, mol):\n",
    "        return Descriptors.ExactMolWt(mol)\n",
    "\n",
    "    def TPSA(self, mol):\n",
    "        return Chem.rdMolDescriptors.CalcTPSA(mol)\n",
    "    \n",
    "    def LASA(self, mol):\n",
    "        return Chem.rdMolDescriptors.CalcLabuteASA(mol)\n",
    "\n",
    "    def NumRotatableBonds(self, mol):\n",
    "        return Descriptors.NumRotatableBonds(mol)\n",
    "\n",
    "    def NumHDonors(self, mol):\n",
    "        return Descriptors.NumHDonors(mol)\n",
    "\n",
    "    def NumHAcceptors(self, mol):\n",
    "        return Descriptors.NumHAcceptors(mol)\n",
    "\n",
    "    def MolLogP(self, mol):\n",
    "        return Descriptors.MolLogP(mol)\n",
    "\n",
    "    def MolMR(self, mol):\n",
    "        return Descriptors.MolMR(mol)\n",
    "\n",
    "    def AromProp(self, mol):\n",
    "        return len(list(mol.GetAromaticAtoms())) / mol.GetNumHeavyAtoms()\n",
    "\n",
    "    def __call__(self, mol: Chem.Mol) -> np.ndarray:\n",
    "        return np.array([getattr(self,feature)(mol) for feature in self.features])\n",
    "\n",
    "class MulticomponentTrainingBatch(NamedTuple):\n",
    "    bmgs: list[BatchMolGraph]\n",
    "    V_ds: list[Tensor | None]\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None\n",
    "\n",
    "\n",
    "def custom_collate_multicomponent(batches: Iterable[Iterable[Datum]]) -> MulticomponentTrainingBatch:\n",
    "    tbs = [collate_batch(batch) for batch in zip(*batches)]\n",
    "    return MulticomponentTrainingBatch(\n",
    "        [tb.bmg for tb in tbs],\n",
    "        [tb.V_d for tb in tbs],\n",
    "        torch.cat([tbs[0].X_d,tbs[1].X_d],axis=1),\n",
    "        tbs[0].Y,\n",
    "        tbs[0].w,\n",
    "        tbs[0].lt_mask,\n",
    "        tbs[0].gt_mask,\n",
    "    )\n",
    "\n",
    "def custom_build_dataloader(\n",
    "    dataset: MoleculeDataset | ReactionDataset | MulticomponentDataset,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 0,\n",
    "    class_balance: bool = False,\n",
    "    seed: int | None = None,\n",
    "    shuffle: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    if class_balance:\n",
    "        sampler = ClassBalanceSampler(dataset.Y, seed, shuffle)\n",
    "    elif shuffle and seed is not None:\n",
    "        sampler = SeededSampler(len(dataset), seed)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    if isinstance(dataset, MulticomponentDataset):\n",
    "        collate_fn = custom_collate_multicomponent\n",
    "    else:\n",
    "        collate_fn = collate_batch\n",
    "\n",
    "    if len(dataset) % batch_size == 1:\n",
    "        warnings.warn(\n",
    "            f\"Dropping last batch of size 1 to avoid issues with batch normalization \\\n",
    "(dataset size = {len(dataset)}, batch_size = {batch_size})\"\n",
    "        )\n",
    "        drop_last = True\n",
    "    else:\n",
    "        drop_last = False\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        sampler is None and shuffle,\n",
    "        sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=drop_last,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "features = [\"TPSA\", \"LASA\", \"NumRotatableBonds\", \"NumHDonors\", \"NumHAcceptors\", \"MolLogP\", \"MolMR\", \"AromProp\"]\n",
    "\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "train_file = data_dir / \"solvation_train.csv\"\n",
    "test_file = data_dir / \"solvation_test.csv\"\n",
    "prop_file = data_dir / \"molecule_props.csv\"\n",
    "smiles_columns = ['Solute', 'Solvent'] # name of the column containing SMILES strings\n",
    "target_columns = ['logK'] # list of names of the columns containing targets\n",
    "df_input = pd.read_csv(train_file)\n",
    "smiss = df_input.loc[:, smiles_columns].values\n",
    "ys = df_input.loc[:, target_columns].values\n",
    "\n",
    "split_type=\"random\"\n",
    "split = (0.8, 0.2, 0)\n",
    "\n",
    "mfs = [PropFeaturizer(features)]\n",
    "all_data = [[data.MoleculeDatapoint.from_smi(smis[0], y, mfs=mfs) for smis, y in zip(smiss, ys)]]\n",
    "all_data += [[data.MoleculeDatapoint.from_smi(smis[i], mfs=mfs) for smis in smiss] for i in range(1, len(smiles_columns))]\n",
    "\n",
    "component_to_split_by = 0\n",
    "mols = [d.mol for d in all_data[component_to_split_by]]\n",
    "\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(mols, split_type, split)\n",
    "\n",
    "train_data, val_data, test_data = data.split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")\n",
    "\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "train_datasets = [data.MoleculeDataset(train_data[i], featurizer) for i in range(len(smiles_columns))]\n",
    "train_mcdset = data.MulticomponentDataset(train_datasets)\n",
    "scaler = train_mcdset.normalize_targets()\n",
    "train_loader = custom_build_dataloader(train_mcdset)\n",
    "\n",
    "val_datasets = [data.MoleculeDataset(val_data[i], featurizer) for i in range(len(smiles_columns))]\n",
    "val_mcdset = data.MulticomponentDataset(val_datasets)\n",
    "val_mcdset.normalize_targets(scaler)\n",
    "val_loader = custom_build_dataloader(val_mcdset, shuffle=False)\n",
    "\n",
    "test_datasets = [data.MoleculeDataset(test_data[i], featurizer) for i in range(len(smiles_columns))]\n",
    "test_mcdset = data.MulticomponentDataset(test_datasets)\n",
    "test_loader = custom_build_dataloader(test_mcdset, shuffle=False)\n",
    "\n",
    "class MulticomponentMPNN:\n",
    "    def __init__(self, smiles_columns, scaler, features, hidden_dim=1900, n_layers=2, dropout=0.008, depth=6):\n",
    "        # Initialize the Multicomponent Message Passing Neural Network component\n",
    "        self.mcmp = nn.MulticomponentMessagePassing(\n",
    "            blocks=[nn.BondMessagePassing(depth=depth) for _ in range(len(smiles_columns))],\n",
    "            n_components=len(smiles_columns),\n",
    "        )\n",
    "\n",
    "        # Initialize the aggregation method\n",
    "        self.agg = nn.MeanAggregation()\n",
    "\n",
    "        # Output transform setup\n",
    "        self.output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "\n",
    "        # Fully connected feedforward network\n",
    "        self.ffn = nn.RegressionFFN(\n",
    "            input_dim=self.mcmp.output_dim + 2 * len(features),\n",
    "            output_transform=self.output_transform,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Metric list setup\n",
    "        self.metric_list = [nn.metrics.RMSEMetric(), nn.metrics.MAEMetric(), nn.metrics.R2Metric()]\n",
    "\n",
    "        # Final MPNN model composition\n",
    "        self.model = models.multi.MulticomponentMPNN(\n",
    "            self.mcmp,\n",
    "            self.agg,\n",
    "            self.ffn,\n",
    "            metrics=self.metric_list,\n",
    "        )\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 100, 2400)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    depth = trial.suggest_int(\"depth\", 2, 6)\n",
    "    max_epochs = trial.suggest_int(\"max_epochs\", 50, 200)\n",
    "\n",
    "    mcmpnn = MulticomponentMPNN(smiles_columns, scaler, features, hidden_dim=1900, n_layers=2, dropout=0.008, depth=6)\n",
    "\n",
    "    logger = CSVLogger(\"logs\", name=\"hyperparameter\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
    "        enable_progress_bar=False,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=2,\n",
    "        max_epochs=max_epochs, # number of epochs to train for\n",
    "    )\n",
    "\n",
    "    hyperparameters = dict(hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout, depth=depth, max_epochs=max_epochs)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(mcmpnn.get_model(), train_loader, val_loader)  # Define your dataloaders properly\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()  # Or any other metric that you aim to minimize\n",
    "\n",
    "# Create a study and execute optimization\n",
    "pruner = optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(), patience=1)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
