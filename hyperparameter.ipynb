{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo8WznxNdqD9"
      },
      "source": [
        "#  <center> Problem Set 6 <center>\n",
        "\n",
        "<center> 3.C01/3.C51, 10.C01/10.C51 <center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkB_bDgydqEJ"
      },
      "source": [
        "## Part 1: Baseline Regression Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZBbezSgeV9i"
      },
      "source": [
        "### Part 1.1: (5 points) Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5MSTRFRzdqEL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv('./solvation_train.csv')\n",
        "mol_prop = pd.read_csv('./molecule_props.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok9EthEXec48"
      },
      "source": [
        "Some utility functions for you to generate features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CHKmkNepdqER"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem, Descriptors,Crippen\n",
        "from rdkit import RDLogger\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from package.plot import get_size_inches\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from lightning import pytorch as pl\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "from chemprop import data, featurizers, models, nn\n",
        "\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "\n",
        "RDLogger.DisableLog('rdApp.*')         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMYXzNLCepen"
      },
      "source": [
        "Generate fingerprints (e.g. a Morgan fingerprint)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npN-qSQDdqER"
      },
      "source": [
        "## Part 2: (50 points) Machine Learning Competition and Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J34YVX-nkoJd"
      },
      "source": [
        "You can start a new notebook here to put all your models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j3B9eYwVdqEd"
      },
      "outputs": [],
      "source": [
        "def save_submission(prediction, filename):\n",
        "    '''\n",
        "    Utility function to dump a submission file.\n",
        "\n",
        "    prediction (numpy.array): 1d numpy array contains your prediction\n",
        "    filename (str): file path to where you want to save the result\n",
        "    '''\n",
        "    sub = pd.DataFrame( {'index': list(range(len(prediction))), 'logK': prediction } )\n",
        "    sub.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "chemprop_dir = Path.cwd()\n",
        "input_path = chemprop_dir / \"solvation_train.csv\"\n",
        "smiles_columns = ['Solute', 'Solvent'] # name of the column containing SMILES strings\n",
        "target_columns = ['logK'] # list of names of the columns containing targets\n",
        "df_input = pd.read_csv(input_path)\n",
        "smiss = df_input.loc[:, smiles_columns].values\n",
        "ys = df_input.loc[:, target_columns].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PropFeaturizer(featurizers.Featurizer):\n",
        "    size = 6\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"the length of the feature vector\"\"\"\n",
        "        return self.size\n",
        "\n",
        "    def __call__(self, mol: Chem.Mol) -> np.ndarray:\n",
        "        \"\"\"Featurize the molecule ``mol``\"\"\"\n",
        "\n",
        "        MolWt = Descriptors.ExactMolWt(mol)\n",
        "        TPSA = Chem.rdMolDescriptors.CalcTPSA(mol) #Topological Polar Surface Area\n",
        "        nRotB = Descriptors.NumRotatableBonds(mol) #Number of rotable bonds\n",
        "        HBD = Descriptors.NumHDonors(mol) #Number of H bond donors\n",
        "        HBA = Descriptors.NumHAcceptors(mol) #Number of H bond acceptors\n",
        "        logP = Descriptors.MolLogP(mol) #LogP\n",
        "        \n",
        "        return [MolWt, TPSA, nRotB, HBD, HBA, logP]\n",
        "    \n",
        "    # def __call__(self, mol: Chem.Mol) -> np.ndarray:\n",
        "    #     \"\"\"Featurize the molecule ``mol``\"\"\"\n",
        "        \n",
        "    #     # define Mol object\n",
        "    #     mol = Chem.MolFromSmiles(smiles)\n",
        "        \n",
        "    #     # get morgan fingerprint\n",
        "    #     # obtain a 512 bit fingperint, with radius 2\n",
        "    #     fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=512)\n",
        "\n",
        "    #     # convert to numpy array\n",
        "    #     fp_array = np.zeros((1,), int)\n",
        "    #     DataStructs.ConvertToNumpyArray(fp, fp_array)\n",
        "\n",
        "    #     MolWt = Descriptors.ExactMolWt(mol)\n",
        "    #     TPSA = Chem.rdMolDescriptors.CalcTPSA(mol) #Topological Polar Surface Area\n",
        "    #     nRotB = Descriptors.NumRotatableBonds(mol) #Number of rotable bonds\n",
        "    #     HBD = Descriptors.NumHDonors(mol) #Number of H bond donors\n",
        "    #     HBA = Descriptors.NumHAcceptors(mol) #Number of H bond acceptors\n",
        "    #     logP = Descriptors.MolLogP(mol) #LogP\n",
        "        \n",
        "    #     return np.hstack([fp_array, [MolWt, TPSA, nRotB, HBD, HBA, logP]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chemprop.data.collate import *\n",
        "from chemprop.data.dataloader import *\n",
        "\n",
        "class MulticomponentTrainingBatch(NamedTuple):\n",
        "    bmgs: list[BatchMolGraph]\n",
        "    V_ds: list[Tensor | None]\n",
        "    X_d: Tensor | None\n",
        "    Y: Tensor | None\n",
        "    w: Tensor\n",
        "    lt_mask: Tensor | None\n",
        "    gt_mask: Tensor | None\n",
        "\n",
        "\n",
        "def custom_collate_multicomponent(batches: Iterable[Iterable[Datum]]) -> MulticomponentTrainingBatch:\n",
        "    tbs = [collate_batch(batch) for batch in zip(*batches)]\n",
        "    return MulticomponentTrainingBatch(\n",
        "        [tb.bmg for tb in tbs],\n",
        "        [tb.V_d for tb in tbs],\n",
        "        torch.cat([tbs[0].X_d,tbs[1].X_d],axis=1),\n",
        "        tbs[0].Y,\n",
        "        tbs[0].w,\n",
        "        tbs[0].lt_mask,\n",
        "        tbs[0].gt_mask,\n",
        "    )\n",
        "\n",
        "def custom_build_dataloader(\n",
        "    dataset: MoleculeDataset | ReactionDataset | MulticomponentDataset,\n",
        "    batch_size: int = 64,\n",
        "    num_workers: int = 0,\n",
        "    class_balance: bool = False,\n",
        "    seed: int | None = None,\n",
        "    shuffle: bool = True,\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "    if class_balance:\n",
        "        sampler = ClassBalanceSampler(dataset.Y, seed, shuffle)\n",
        "    elif shuffle and seed is not None:\n",
        "        sampler = SeededSampler(len(dataset), seed)\n",
        "    else:\n",
        "        sampler = None\n",
        "\n",
        "    if isinstance(dataset, MulticomponentDataset):\n",
        "        collate_fn = custom_collate_multicomponent\n",
        "    else:\n",
        "        collate_fn = collate_batch\n",
        "\n",
        "    if len(dataset) % batch_size == 1:\n",
        "        warnings.warn(\n",
        "            f\"Dropping last batch of size 1 to avoid issues with batch normalization \\\n",
        "(dataset size = {len(dataset)}, batch_size = {batch_size})\"\n",
        "        )\n",
        "        drop_last = True\n",
        "    else:\n",
        "        drop_last = False\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size,\n",
        "        sampler is None and shuffle,\n",
        "        sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=drop_last,\n",
        "        **kwargs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "mfs = [PropFeaturizer()]\n",
        "all_data = [[data.MoleculeDatapoint.from_smi(smis[0], y, mfs=mfs) for smis, y in zip(smiss, ys)]]\n",
        "all_data += [[data.MoleculeDatapoint.from_smi(smis[i], mfs=mfs) for smis in smiss] for i in range(1, len(smiles_columns))]\n",
        "component_to_split_by = 0 # index of the component to use for structure based splits\n",
        "mols = [d.mol for d in all_data[component_to_split_by]]\n",
        "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.9, 0.05, 0.05))\n",
        "train_data, val_data, test_data = data.split_data_by_indices(\n",
        "    all_data, train_indices, val_indices, test_indices\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
        "\n",
        "train_datasets = [data.MoleculeDataset(train_data[i], featurizer) for i in range(len(smiles_columns))]\n",
        "val_datasets = [data.MoleculeDataset(val_data[i], featurizer) for i in range(len(smiles_columns))]\n",
        "test_datasets = [data.MoleculeDataset(test_data[i], featurizer) for i in range(len(smiles_columns))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_mcdset = data.MulticomponentDataset(train_datasets)\n",
        "scaler = train_mcdset.normalize_targets()\n",
        "val_mcdset = data.MulticomponentDataset(val_datasets)\n",
        "val_mcdset.normalize_targets(scaler)\n",
        "test_mcdset = data.MulticomponentDataset(test_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = custom_build_dataloader(train_mcdset)\n",
        "val_loader = custom_build_dataloader(val_mcdset, shuffle=False)\n",
        "test_loader = custom_build_dataloader(test_mcdset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-05-03 22:01:13,375] A new study created in memory with name: no-name-f49d2031-a894-4b6f-b48c-1cb2f5b49649\n",
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/gridsan/ddavid/.conda/envs/torch/lib/python3.1 ...\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/gridsan/ddavid/.conda/envs/torch/lib/python3.1 ...\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n",
            "\n",
            "  | Name            | Type                         | Params\n",
            "-----------------------------------------------------------------\n",
            "0 | message_passing | MulticomponentMessagePassing | 455 K \n",
            "1 | agg             | MeanAggregation              | 0     \n",
            "2 | bn              | BatchNorm1d                  | 1.2 K \n",
            "3 | predictor       | RegressionFFN                | 887 K \n",
            "4 | X_d_transform   | Identity                     | 0     \n",
            "-----------------------------------------------------------------\n",
            "1.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.3 M     Total params\n",
            "5.375     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54: 100%|██████████| 52/52 [00:01<00:00, 36.57it/s, v_num=2, train_loss=0.0153, val_loss=0.119] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=55` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54: 100%|██████████| 52/52 [00:01<00:00, 35.53it/s, v_num=2, train_loss=0.0153, val_loss=0.119]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-05-03 22:02:36,854] Trial 0 finished with value: 0.1189720556139946 and parameters: {'hidden_dim': 1445, 'n_layers': 1, 'dropout': 0.317864187524596, 'depth': 6, 'max_epochs': 55}. Best is trial 0 with value: 0.1189720556139946.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\n",
            "  | Name            | Type                         | Params\n",
            "-----------------------------------------------------------------\n",
            "0 | message_passing | MulticomponentMessagePassing | 455 K \n",
            "1 | agg             | MeanAggregation              | 0     \n",
            "2 | bn              | BatchNorm1d                  | 1.2 K \n",
            "3 | predictor       | RegressionFFN                | 3.9 M \n",
            "4 | X_d_transform   | Identity                     | 0     \n",
            "-----------------------------------------------------------------\n",
            "4.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.3 M     Total params\n",
            "17.227    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40:  79%|███████▉  | 41/52 [00:01<00:00, 38.54it/s, v_num=2, train_loss=0.024, val_loss=0.247]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gridsan/ddavid/.conda/envs/torch/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "[I 2024-05-03 22:03:40,791] Trial 1 finished with value: 0.24684648215770721 and parameters: {'hidden_dim': 1242, 'n_layers': 3, 'dropout': 0.27583313578022767, 'depth': 5, 'max_epochs': 165}. Best is trial 0 with value: 0.1189720556139946.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\n",
            "  | Name            | Type                         | Params\n",
            "-----------------------------------------------------------------\n",
            "0 | message_passing | MulticomponentMessagePassing | 455 K \n",
            "1 | agg             | MeanAggregation              | 0     \n",
            "2 | bn              | BatchNorm1d                  | 1.2 K \n",
            "3 | predictor       | RegressionFFN                | 2.7 M \n",
            "4 | X_d_transform   | Identity                     | 0     \n",
            "-----------------------------------------------------------------\n",
            "3.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.2 M     Total params\n",
            "12.744    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7:  25%|██▌       | 13/52 [00:00<00:01, 38.50it/s, v_num=0, train_loss=0.0273, val_loss=0.195]"
          ]
        }
      ],
      "source": [
        "def objective(trial: optuna.Trial) -> float:\n",
        "    # Define hyperparameters using trial object\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 100, 2400)\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
        "    depth = trial.suggest_int(\"depth\", 2, 6)\n",
        "    max_epochs = trial.suggest_int(\"max_epochs\", 50, 200)\n",
        "\n",
        "    # Model setup (Using your existing setup)\n",
        "    mcmp = nn.MulticomponentMessagePassing(\n",
        "        blocks=[nn.BondMessagePassing(depth=depth) for _ in range(len(smiles_columns))],\n",
        "        n_components=len(smiles_columns),\n",
        "    )\n",
        "    agg = nn.MeanAggregation()\n",
        "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
        "    ffn = nn.RegressionFFN(\n",
        "        input_dim=mcmp.output_dim + (len(smiles_columns)) * np.sum([i.__len__() for i in mfs]),\n",
        "        hidden_dim=hidden_dim,\n",
        "        n_layers=n_layers,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "    model = models.multi.MulticomponentMPNN(mcmp, agg, ffn, metrics=[nn.metrics.RMSEMetric(), nn.metrics.MAEMetric(), nn.metrics.R2Metric()])\n",
        "\n",
        "    # Logger and trainer setup\n",
        "    logger = CSVLogger('logs', name=f'hyper_{trial.number}')\n",
        "    trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
        "        enable_checkpointing=True,\n",
        "        enable_progress_bar=True,\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        max_epochs=max_epochs,\n",
        "    )\n",
        "\n",
        "    hyperparameters = dict(hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout, depth=depth, max_epochs=max_epochs)\n",
        "    trainer.logger.log_hyperparams(hyperparameters)\n",
        "    trainer.fit(model, train_loader, val_loader)  # Define your dataloaders properly\n",
        "\n",
        "    return trainer.callback_metrics[\"val_loss\"].item()  # Or any other metric that you aim to minimize\n",
        "\n",
        "# Create a study and execute optimization\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=300)  # You can adjust the number of trials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pset6_template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
